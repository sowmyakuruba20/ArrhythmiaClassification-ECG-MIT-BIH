# -*- coding: utf-8 -*-
"""Attention-based CNN+BiGRU.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MPDxXvZezFcENcqon7g3I__EO6zX7sf6
"""

!pip install wfdb

# Import necessary libraries
import wfdb
from google.colab import drive
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, GRU, Dropout, MaxPooling1D, Flatten, Bidirectional, Layer
from tensorflow.keras.regularizers import l2
from scipy.signal import find_peaks
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
import os

# Define the Attention Layer
class Attention(Layer):
    def __init__(self, **kwargs):
        super(Attention, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], 1),
                                 initializer='random_normal', trainable=True)
        self.b = self.add_weight(name='attention_bias', shape=(input_shape[1], 1),
                                 initializer='zeros', trainable=True)
        super(Attention, self).build(input_shape)

    def call(self, x):
        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)
        a = tf.keras.backend.softmax(e, axis=1)
        output = x * a
        return tf.keras.backend.sum(output, axis=1)

# Mount Google Drive
drive.mount('/content/gdrive')
folder_path = '/content/gdrive/My Drive/ECG_GWAR_DATA/mit-bih-arrhythmia-database-1.0.0/'
os.chdir(folder_path)

# Function to preprocess the ECG data
def preprocess_ecg(record_name, atr_file_extension, aami_categories):
    try:
        # Load ECG data from the .dat file
        ecg_data, _ = wfdb.rdsamp(os.path.join(folder_path, record_name), channels=[0])

        # Load annotations from the .atr file
        annotation = wfdb.rdann(os.path.join(folder_path, record_name), atr_file_extension)

        # Normalize the ECG signal to the range [0, 1]
        ecg_data = (ecg_data - np.min(ecg_data)) / (np.max(ecg_data) - np.min(ecg_data))

        # Find R-peaks in the ECG signal
        r_peaks, _ = find_peaks(ecg_data[:, 0], height=0.2, distance=200)

        for r_peak in r_peaks:
            start = r_peak - 100
            end = r_peak + 100
            if start < 0 or end >= len(ecg_data):
                continue

            heartbeat = ecg_data[start:end, 0]
            annotation_indices = np.where(annotation.sample == r_peak)[0]

            if annotation_indices.size > 0:
                annotation_index = annotation_indices[0]
                annotation_label = annotation.symbol[annotation_index]
                aami_category = 'Unknown'  # Default category
                if annotation_label in ['N', 'L', 'R', 'e', 'j']:
                    aami_category = 'N'
                elif annotation_label in ['V', 'E']:
                    aami_category = 'V'
                elif annotation_label in ['A', 'a', 'J', 'S']:
                    aami_category = 'S'
                elif annotation_label == 'F':
                    aami_category = 'F'
                elif annotation_label in ['/', 'f', 'Q', 'U']:
                    aami_category = 'Q'

                aami_categories[aami_category].append(heartbeat)

    except FileNotFoundError:
        print(f"File '{record_name}' not found. Skipping...")

    return aami_categories

# Initialize AAMI categories and process each record
aami_categories = {'N': [], 'S': [], 'V': [], 'F': [], 'Q': [], 'Unknown': []}
all_labels = []
all_heartbeats = []

record_numbers = [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 122, 123, 124, 200, 201, 202, 203, 205, 207, 208, 209, 210, 212, 213, 214, 215, 217, 219, 220, 221, 222, 223, 228, 230, 231, 232, 233, 234]
for record_number in record_numbers:
    aami_categories = preprocess_ecg(str(record_number), 'atr', aami_categories)

for category, heartbeats in aami_categories.items():
    all_heartbeats.extend(heartbeats)
    all_labels.extend([category] * len(heartbeats))

# Convert lists to NumPy arrays
all_heartbeats = np.array(all_heartbeats)
all_labels = np.array(all_labels)



# Flatten heartbeats and labels for model input
flattened_heartbeats, flattened_labels = [], []
for category, hb in aami_categories.items():
    flattened_heartbeats.extend(hb)
    flattened_labels.extend([category] * len(hb))

# Encode labels and prepare data
encoder = LabelEncoder()
all_labels_encoded = encoder.fit_transform(flattened_labels)
X = np.expand_dims(flattened_heartbeats, axis=2)
y = tf.keras.utils.to_categorical(all_labels_encoded)

# Apply SMOTE
smote = SMOTE()
X_resampled, y_resampled = smote.fit_resample(X.reshape(X.shape[0], -1), np.argmax(y, axis=1))
X_resampled = X_resampled.reshape(-1, X.shape[1], 1)
y_resampled = tf.keras.utils.to_categorical(y_resampled)

# Split the data
X_train_val, X_test, y_train_val, y_test = train_test_split(X_resampled, y_resampled, test_size=0.15, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.15/0.85, random_state=42)

# Define and build the model
l2_rate = 0.0001
model = Sequential()
model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.05))
model.add(Bidirectional(GRU(100, return_sequences=True)))
model.add(Attention())
model.add(Flatten())
model.add(Dense(100, activation='relu', kernel_regularizer=l2(l2_rate)))
model.add(Dense(y_resampled.shape[1], activation='softmax', kernel_regularizer=l2(l2_rate)))

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val))

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {test_accuracy*100:.2f}%')

# Extract training and validation accuracies
training_accuracy = history.history['accuracy'][-1]
validation_accuracy = history.history['val_accuracy'][-1]
print(f'Training Accuracy: {training_accuracy*100:.2f}%')
print(f'Validation Accuracy: {validation_accuracy*100:.2f}%')

# Generate predictions and classification report
predictions = model.predict(X_test)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = np.argmax(y_test, axis=1)
print(classification_report(true_classes, predicted_classes, target_names=encoder.classes_))

l2_rate = 0.0001
model = Sequential()
model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.05))
model.add(Bidirectional(GRU(100, return_sequences=True)))
model.add(Attention())
model.add(Flatten())
model.add(Dense(100, activation='relu', kernel_regularizer=l2(l2_rate)))
model.add(Dense(y_resampled.shape[1], activation='softmax', kernel_regularizer=l2(l2_rate)))

tf.keras.utils.plot_model(model, to_file='model_architecture.png', show_shapes=True, show_layer_names=True)

!pip install wfdb

# Import necessary libraries
import wfdb
from google.colab import drive
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Conv1D, GRU, Dropout, MaxPooling1D, Flatten, Bidirectional, Layer
from tensorflow.keras.regularizers import l2
from scipy.signal import find_peaks
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import numpy as np
import os

# Define the Attention Layer
class Attention(Layer):
    def __init__(self, **kwargs):
        super(Attention, self).__init__(**kwargs)

    def build(self, input_shape):
        self.W = self.add_weight(name='attention_weight', shape=(input_shape[-1], 1),
                                 initializer='random_normal', trainable=True)
        self.b = self.add_weight(name='attention_bias', shape=(input_shape[1], 1),
                                 initializer='zeros', trainable=True)
        super(Attention, self).build(input_shape)

    def call(self, x):
        e = tf.keras.backend.tanh(tf.keras.backend.dot(x, self.W) + self.b)
        a = tf.keras.backend.softmax(e, axis=1)
        output = x * a
        return tf.keras.backend.sum(output, axis=1)

# Mount Google Drive
drive.mount('/content/gdrive')
folder_path = '/content/gdrive/My Drive/ECG_GWAR_DATA/mit-bih-arrhythmia-database-1.0.0/'
os.chdir(folder_path)

# Function to preprocess the ECG data
def preprocess_ecg(record_name, atr_file_extension, aami_categories):
    try:
        # Load ECG data from the .dat file
        ecg_data, _ = wfdb.rdsamp(os.path.join(folder_path, record_name), channels=[0])

        # Load annotations from the .atr file
        annotation = wfdb.rdann(os.path.join(folder_path, record_name), atr_file_extension)

        # Normalize the ECG signal to the range [0, 1]
        ecg_data = (ecg_data - np.min(ecg_data)) / (np.max(ecg_data) - np.min(ecg_data))

        # Find R-peaks in the ECG signal
        r_peaks, _ = find_peaks(ecg_data[:, 0], height=0.2, distance=200)

        for r_peak in r_peaks:
            start = r_peak - 100
            end = r_peak + 100
            if start < 0 or end >= len(ecg_data):
                continue

            heartbeat = ecg_data[start:end, 0]
            annotation_indices = np.where(annotation.sample == r_peak)[0]

            if annotation_indices.size > 0:
                annotation_index = annotation_indices[0]
                annotation_label = annotation.symbol[annotation_index]
                  # Default category
                if annotation_label in ['N', 'L', 'R', 'e', 'j']:
                    aami_category = 'N'
                elif annotation_label in ['V', 'E']:
                    aami_category = 'V'
                elif annotation_label in ['A', 'a', 'J', 'S']:
                    aami_category = 'S'
                elif annotation_label == 'F':
                    aami_category = 'F'
                elif annotation_label in ['/', 'f', 'Q', 'U']:
                    aami_category = 'Q'

                aami_categories[aami_category].append(heartbeat)

    except FileNotFoundError:
        print(f"File '{record_name}' not found. Skipping...")

    return aami_categories

# Initialize AAMI categories and process each record
aami_categories = {'N': [], 'S': [], 'V': [], 'F': [], 'Q': []}
all_labels = []
all_heartbeats = []

record_numbers = [100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 118, 119, 121, 122, 123, 124, 200, 201, 202, 203, 205, 207, 208, 209, 210, 212, 213, 214, 215, 217, 219, 220, 221, 222, 223, 228, 230, 231, 232, 233, 234]
for record_number in record_numbers:
    aami_categories = preprocess_ecg(str(record_number), 'atr', aami_categories)

for category, heartbeats in aami_categories.items():
    all_heartbeats.extend(heartbeats)
    all_labels.extend([category] * len(heartbeats))

# Convert lists to NumPy arrays
all_heartbeats = np.array(all_heartbeats)
all_labels = np.array(all_labels)



# Flatten heartbeats and labels for model input
flattened_heartbeats, flattened_labels = [], []
for category, hb in aami_categories.items():
    flattened_heartbeats.extend(hb)
    flattened_labels.extend([category] * len(hb))

# Encode labels and prepare data
encoder = LabelEncoder()
all_labels_encoded = encoder.fit_transform(flattened_labels)
X = np.expand_dims(flattened_heartbeats, axis=2)
y = tf.keras.utils.to_categorical(all_labels_encoded)

# Apply SMOTE
smote = SMOTE()
X_resampled, y_resampled = smote.fit_resample(X.reshape(X.shape[0], -1), np.argmax(y, axis=1))
X_resampled = X_resampled.reshape(-1, X.shape[1], 1)
y_resampled = tf.keras.utils.to_categorical(y_resampled)

# Split the data
X_train_val, X_test, y_train_val, y_test = train_test_split(X_resampled, y_resampled, test_size=0.15, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.15/0.85, random_state=42)

# Define and build the model
l2_rate = 0.00001
model = Sequential()
model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))
model.add(MaxPooling1D(pool_size=2))
model.add(Dropout(0.5))
model.add(Bidirectional(GRU(100, return_sequences=True)))
model.add(Attention())
model.add(Flatten())
model.add(Dense(100, activation='relu', kernel_regularizer=l2(l2_rate)))
model.add(Dense(y_resampled.shape[1], activation='softmax', kernel_regularizer=l2(l2_rate)))

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_val, y_val))

# Evaluate the model
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print(f'Test Accuracy: {test_accuracy*100:.2f}%')

# Extract training and validation accuracies
training_accuracy = history.history['accuracy'][-1]
validation_accuracy = history.history['val_accuracy'][-1]
print(f'Training Accuracy: {training_accuracy*100:.2f}%')
print(f'Validation Accuracy: {validation_accuracy*100:.2f}%')

# Generate predictions and classification report
predictions = model.predict(X_test)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = np.argmax(y_test, axis=1)
print(classification_report(true_classes, predicted_classes, target_names=encoder.classes_))

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt


# Generate predictions
predictions = model.predict(X_test)
predicted_classes = np.argmax(predictions, axis=1)
true_classes = np.argmax(y_test, axis=1)

# Convert the one-hot encoded classes back to their original label encoding
true_labels = encoder.inverse_transform(true_classes)
predicted_labels = encoder.inverse_transform(predicted_classes)

# Calculate the confusion matrix
cm = confusion_matrix(true_labels, predicted_labels, labels=np.unique(true_labels))

# Normalize the confusion matrix to display percentages
cm_percentage = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100

# Plotting the normalized confusion matrix
plt.figure(figsize=(10, 7))
sns.heatmap(cm_percentage, annot=True, fmt='.2f', cmap='Blues',
            xticklabels=np.unique(true_labels), yticklabels=np.unique(true_labels))

# Add titles and labels
plt.title('Attention-Based CNN-BiGRU: Confusion Matrix')
plt.ylabel('Actual Classes')
plt.xlabel('Predicted Classes')

# Show the plot
plt.show()

from sklearn.metrics import roc_curve, auc, roc_auc_score
from sklearn.preprocessing import label_binarize
import numpy as np
import matplotlib.pyplot as plt
from itertools import cycle
from scipy import interp

# Your class labels (replace with your actual class labels)
class_labels = ['N', 'S', 'V', 'F', 'Q']  # Replace with your arrhythmia class labels

# Binarize the true classes for ROC curve calculation
y_test_binarized = label_binarize(true_classes, classes=np.arange(len(class_labels)))

# Compute ROC curve and ROC area for each class
fpr = dict()
tpr = dict()
roc_auc = dict()
n_classes = len(class_labels)

for i in range(n_classes):
    fpr[i], tpr[i], _ = roc_curve(y_test_binarized[:, i], predictions[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])

# Compute macro-average ROC curve and ROC area
all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))
mean_tpr = np.zeros_like(all_fpr)
for i in range(n_classes):
    mean_tpr += interp(all_fpr, fpr[i], tpr[i])

mean_tpr /= n_classes
fpr["macro"] = all_fpr
tpr["macro"] = mean_tpr
roc_auc["macro"] = auc(fpr["macro"], tpr["macro"])

# Plot all ROC curves
plt.figure(figsize=(8, 6))

colors = cycle(['blue', 'red', 'green', 'orange', 'purple'])
for i, color in zip(range(n_classes), colors):
    plt.plot(fpr[i], tpr[i], color=color, lw=2,
             label='ROC curve of {0} (area = {1:0.2f})'.format(class_labels[i], roc_auc[i]))

plt.plot([0, 1], [0, 1], 'k--', lw=2)
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Attention-Based CNN-BiGRU: Multi-Class ROC Analysis')
plt.legend(loc="lower right")
plt.show()

# Plot training & validation accuracy values
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], color='blue', label='Train')
plt.plot(history.history['val_accuracy'], color='red', label='Validation')
plt.title('Model Accuracy of Attention-Based CNN-BiGRU')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='upper left')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], color='green', label='Train')
plt.plot(history.history['val_loss'], color='orange', label='Validation')
plt.title('Model Loss of Attention-Based CNN-BiGRU')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper left')

plt.show()

# Plot training & validation accuracy values
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], color='blue', label='Train')
plt.plot(history.history['val_accuracy'], color='red', label='Validation')
plt.title('Model Accuracy of Attention-Based CNN-BiGRU')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(loc='lower right')

# Plot training & validation loss values
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], color='green', label='Train')
plt.plot(history.history['val_loss'], color='orange', label='Validation')
plt.title('Model Loss of Attention-Based CNN-BiGRU')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(loc='upper right')

plt.show()